{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-04T23:57:15.423760Z","iopub.status.idle":"2024-05-04T23:57:15.424663Z","shell.execute_reply.started":"2024-05-04T23:57:15.424442Z","shell.execute_reply":"2024-05-04T23:57:15.424463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nimport pandas as pd\n\nimport torch.nn as nn\nclass Layer(nn.Module):#оголошення класу\n    def __init__(self, inputs, outputs, activation_func=None):#виклик конструктора,активація по дефолту не задана\n        super(Layer, self).__init__()#Виклик конструктора батьківського класу\n        self.linear = nn.Linear(inputs, outputs)# Створення об'єкту лінійного шару з визначеними кількостями вхідних та вихідних вузлів\n        self.activation_func = activation_func if activation_func is not None else nn.Identity()#Ініціалізація функції активації. Якщо вона задана, використовується передана функція активації, інакше використовується nn.Identity(), яка не змінює вхідний сигнал.\n\n    def forward(self, x):#метод forward, який викликається під час прямого проходу через мережу.\n        x = self.linear(x)#Застосування лінійного перетворення, представленого nn.Linear, до вхідного тензору x\n        x = self.activation_func(x)#Застосування функції активації до результату лінійного перетворення\n        return x#\n\nclass MLP(nn.Module):#оголошення класу\n    def __init__(self, layer_sizes, activation_funcs=None):#конструктор приймає список розмірів шарів у мережі (layer_sizes) , список функцій активації для кожного шару (якщо є)\n        super(MLP, self).__init__()#конструктора батьківського класу\n        if activation_funcs is None:#Перевірка, чи заданий список функцій активації. Якщо він не заданий (None), встановлюється список з однаковими значеннями None, крім останнього шару (за замовчуванням функція активації не застосовується до останнього шару).\n            activation_funcs = [None] * (len(layer_sizes) - 1)  # Default to no activation\n        self.layers = nn.ModuleList([Layer(layer_sizes[i], layer_sizes[i+1], activation_funcs[i]) #створення шарів\n                                     for i in range(len(layer_sizes) - 1)])#Цикл, який проходить по всім шарам крім останнього.\n\n    def forward(self, x):#метод forward, який викликається під час прямого проходу через мережу.\n        for layer in self.layers:#Подача вхідного тензору x через кожний шар мережі. Кожний шар обробляє вхід і повертає вихід, який стає новим вхідним тензором для наступного шару.\n            x = layer(x)\n        return x\n\n# Завантаження датасету Iris\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Скоригування масштабу даних\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Конвертація в тензори PyTorch\nX_tensor = torch.tensor(X, dtype=torch.float32)\ny_tensor = torch.tensor(y, dtype=torch.long)\n\n# Розділення даних на тренувальний та тестовий набори\nX_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n\n# Створення об'єктів TensorDataset для тренувального та тестового наборів\ntrain_dataset = TensorDataset(X_train, y_train)\ntest_dataset = TensorDataset(X_test, y_test)\n\n# Створення Dataloader для тренувального та тестового наборів\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\ndef train(model, criterion, optimizer, train_loader, num_epochs=10):\n    model.train()  # Встановлюємо модель у режим навчання\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            optimizer.zero_grad()  # Обнуляємо градієнти\n            outputs = model(inputs)  # Прямий прохід\n            loss = criterion(outputs, labels)  # Обрахунок втрат\n            loss.backward()  # Обчислення градієнтів\n            optimizer.step()  # Оновлення ваг моделі\n\n            running_loss += loss.item() * inputs.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n\n# Визначення моделі, критерію, оптимізатора та кількості епох(датасет розміром 64*4)\n\nmodel = MLP([4, 16, 3], [torch.relu, torch.relu])\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nnum_epochs = 20\n\n# Навчання моделі\ntrain(model, criterion, optimizer, train_loader, num_epochs)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T01:12:13.115961Z","iopub.execute_input":"2024-05-05T01:12:13.116354Z","iopub.status.idle":"2024-05-05T01:12:13.220479Z","shell.execute_reply.started":"2024-05-05T01:12:13.116324Z","shell.execute_reply":"2024-05-05T01:12:13.219475Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 1.1076\nEpoch 2, Loss: 1.0978\nEpoch 3, Loss: 1.0882\nEpoch 4, Loss: 1.0784\nEpoch 5, Loss: 1.0691\nEpoch 6, Loss: 1.0598\nEpoch 7, Loss: 1.0506\nEpoch 8, Loss: 1.0416\nEpoch 9, Loss: 1.0324\nEpoch 10, Loss: 1.0237\nEpoch 11, Loss: 1.0149\nEpoch 12, Loss: 1.0067\nEpoch 13, Loss: 0.9984\nEpoch 14, Loss: 0.9905\nEpoch 15, Loss: 0.9828\nEpoch 16, Loss: 0.9756\nEpoch 17, Loss: 0.9684\nEpoch 18, Loss: 0.9615\nEpoch 19, Loss: 0.9546\nEpoch 20, Loss: 0.9480\n","output_type":"stream"}]}]}